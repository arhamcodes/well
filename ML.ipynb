{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That’s exciting! Here’s a concise guide for preprocessing with scikit-learn:\n",
    "\n",
    "### 1. **Handling Missing Values**\n",
    "   - Use `SimpleImputer` for imputation:\n",
    "     ```python\n",
    "     from sklearn.impute import SimpleImputer\n",
    "     imputer = SimpleImputer(strategy='mean')  # Use 'median', 'most_frequent', or 'constant' as needed\n",
    "     X_filled = imputer.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "### 2. **Scaling Features**\n",
    "   - Standardize or normalize the features:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "     scaler = StandardScaler()  # For normal distribution-like data\n",
    "     # scaler = MinMaxScaler()  # For range-bound data\n",
    "     X_scaled = scaler.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "### 3. **Encoding Categorical Variables**\n",
    "   - One-hot encoding for non-ordinal categories:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import OneHotEncoder\n",
    "     encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "     X_encoded = encoder.fit_transform(X)\n",
    "     ```\n",
    "   - Label encoding for ordinal categories:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import LabelEncoder\n",
    "     le = LabelEncoder()\n",
    "     y_encoded = le.fit_transform(y)\n",
    "     ```\n",
    "\n",
    "### 4. **Feature Engineering**\n",
    "   - Generate new features, interactions, or polynomial terms:\n",
    "     ```python\n",
    "     from sklearn.preprocessing import PolynomialFeatures\n",
    "     poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "     X_poly = poly.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "### 5. **Feature Selection**\n",
    "   - Identify and select important features:\n",
    "     ```python\n",
    "     from sklearn.feature_selection import SelectKBest, f_classif\n",
    "     selector = SelectKBest(score_func=f_classif, k=10)\n",
    "     X_selected = selector.fit_transform(X, y)\n",
    "     ```\n",
    "\n",
    "### 6. **Pipeline for Streamlining**\n",
    "   - Combine preprocessing steps into a pipeline:\n",
    "     ```python\n",
    "     from sklearn.pipeline import Pipeline\n",
    "     pipeline = Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy='mean')),\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('encoder', OneHotEncoder(handle_unknown='ignore'))\n",
    "     ])\n",
    "     X_preprocessed = pipeline.fit_transform(X)\n",
    "     ```\n",
    "\n",
    "### 7. **Testing Multiple Preprocessing Methods**\n",
    "   - Use `GridSearchCV` or `RandomizedSearchCV` to find the best combination of preprocessing steps and model hyperparameters.\n",
    "\n",
    "The best model depends on your dataset's size, complexity, and characteristics, but here are some of the most effective models in general:\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Regression**\n",
    "**Best Models:**\n",
    "1. **Linear Regression**\n",
    "   - Use for small datasets or when relationships are linear.\n",
    "   ```python\n",
    "   from sklearn.linear_model import LinearRegression\n",
    "   model = LinearRegression()\n",
    "   ```\n",
    "\n",
    "2. **Ridge/Lasso Regression**\n",
    "   - Regularization for preventing overfitting.\n",
    "   ```python\n",
    "   from sklearn.linear_model import Ridge, Lasso\n",
    "   model = Ridge(alpha=1.0)  # Lasso(alpha=1.0) for sparsity\n",
    "   ```\n",
    "\n",
    "3. **Random Forest Regressor**\n",
    "   - Non-linear, handles missing data well.\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestRegressor\n",
    "   model = RandomForestRegressor(n_estimators=100)\n",
    "   ```\n",
    "\n",
    "4. **Gradient Boosting (XGBoost/LightGBM/CatBoost)**\n",
    "   - Excels in complex datasets.\n",
    "   ```python\n",
    "   from xgboost import XGBRegressor\n",
    "   model = XGBRegressor()\n",
    "   ```\n",
    "\n",
    "5. **Support Vector Regressor (SVR)**\n",
    "   - Works well for smaller datasets with non-linear relationships.\n",
    "   ```python\n",
    "   from sklearn.svm import SVR\n",
    "   model = SVR(kernel='rbf')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Classification**\n",
    "**Best Models:**\n",
    "1. **Logistic Regression**\n",
    "   - Simple and effective for binary classification.\n",
    "   ```python\n",
    "   from sklearn.linear_model import LogisticRegression\n",
    "   model = LogisticRegression()\n",
    "   ```\n",
    "\n",
    "2. **Random Forest Classifier**\n",
    "   - Robust, interpretable, and handles categorical features.\n",
    "   ```python\n",
    "   from sklearn.ensemble import RandomForestClassifier\n",
    "   model = RandomForestClassifier(n_estimators=100)\n",
    "   ```\n",
    "\n",
    "3. **Gradient Boosting (XGBoost/LightGBM/CatBoost)**\n",
    "   - State-of-the-art for structured data.\n",
    "   ```python\n",
    "   from lightgbm import LGBMClassifier\n",
    "   model = LGBMClassifier()\n",
    "   ```\n",
    "\n",
    "4. **Support Vector Classifier (SVC)**\n",
    "   - Effective for high-dimensional, non-linear datasets.\n",
    "   ```python\n",
    "   from sklearn.svm import SVC\n",
    "   model = SVC(kernel='rbf')\n",
    "   ```\n",
    "\n",
    "5. **k-Nearest Neighbors (kNN)**\n",
    "   - Works well for smaller datasets.\n",
    "   ```python\n",
    "   from sklearn.neighbors import KNeighborsClassifier\n",
    "   model = KNeighborsClassifier(n_neighbors=5)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Clustering**\n",
    "**Best Models:**\n",
    "1. **k-Means**\n",
    "   - Simple and scalable, good for spherical clusters.\n",
    "   ```python\n",
    "   from sklearn.cluster import KMeans\n",
    "   model = KMeans(n_clusters=3)\n",
    "   ```\n",
    "\n",
    "2. **DBSCAN**\n",
    "   - Handles arbitrary shapes and noise.\n",
    "   ```python\n",
    "   from sklearn.cluster import DBSCAN\n",
    "   model = DBSCAN(eps=0.5, min_samples=5)\n",
    "   ```\n",
    "\n",
    "3. **Agglomerative Clustering**\n",
    "   - Hierarchical approach, no need for predefined clusters.\n",
    "   ```python\n",
    "   from sklearn.cluster import AgglomerativeClustering\n",
    "   model = AgglomerativeClustering(n_clusters=3)\n",
    "   ```\n",
    "\n",
    "4. **Gaussian Mixture Models (GMM)**\n",
    "   - Probabilistic approach, good for overlapping clusters.\n",
    "   ```python\n",
    "   from sklearn.mixture import GaussianMixture\n",
    "   model = GaussianMixture(n_components=3)\n",
    "   ```\n",
    "\n",
    "5. **Spectral Clustering**\n",
    "   - Effective for non-convex clusters.\n",
    "   ```python\n",
    "   from sklearn.cluster import SpectralClustering\n",
    "   model = SpectralClustering(n_clusters=3, affinity='nearest_neighbors')\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Tips to Choose:\n",
    "- **Regression:** Start with `LinearRegression` and move to `XGBRegressor` or `RandomForestRegressor` for non-linear cases.\n",
    "- **Classification:** Use `LogisticRegression` or `RandomForestClassifier` for interpretability and LightGBM for large datasets.\n",
    "- **Clustering:** Try `k-Means` first, and then experiment with `DBSCAN` or `GMM` if clusters are irregular or overlapping.\n",
    "\n",
    "Here’s a strategic plan to maximize your model's accuracy for the hackathon:\n",
    "\n",
    "---\n",
    "\n",
    "### **General Tips**\n",
    "1. **Understand the Dataset Thoroughly**  \n",
    "   - Inspect data distributions, feature types, missing values, and correlations.\n",
    "   - Use visualization tools like `matplotlib` and `seaborn` to identify trends.\n",
    "\n",
    "2. **Start Simple, Then Iterate**\n",
    "   - Begin with a straightforward model (e.g., Logistic Regression for classification, Linear Regression for regression).\n",
    "   - Quickly iterate with more complex models like Random Forest, Gradient Boosting, etc.\n",
    "\n",
    "3. **Efficient Data Splitting**\n",
    "   - Use stratified splits (for classification) to ensure balanced class distribution:\n",
    "     ```python\n",
    "     from sklearn.model_selection import train_test_split\n",
    "     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "     ```\n",
    "\n",
    "4. **Avoid Data Leakage**\n",
    "   - Ensure that preprocessing steps like scaling or encoding are fit only on the training data, not on the entire dataset.\n",
    "\n",
    "5. **Monitor Model Performance**\n",
    "   - Evaluate using metrics suitable for the task:\n",
    "     - **Regression:** RMSE, MAE, R².\n",
    "     - **Classification:** Accuracy, Precision, Recall, F1-Score, AUC-ROC.\n",
    "\n",
    "---\n",
    "\n",
    "### **Maximizing Accuracy**\n",
    "1. **Feature Engineering**\n",
    "   - Create new features (e.g., interactions, polynomial terms, domain-specific transformations).\n",
    "   - Remove irrelevant or redundant features using correlation analysis or `SelectKBest`.\n",
    "\n",
    "2. **Hyperparameter Tuning**\n",
    "   - Use `GridSearchCV` or `RandomizedSearchCV` for finding the best parameters:\n",
    "     ```python\n",
    "     from sklearn.model_selection import GridSearchCV\n",
    "     grid_search = GridSearchCV(estimator=model, param_grid=params, cv=5, scoring='accuracy')\n",
    "     grid_search.fit(X_train, y_train)\n",
    "     best_model = grid_search.best_estimator_\n",
    "     ```\n",
    "\n",
    "3. **Ensemble Models**\n",
    "   - Combine multiple models to improve predictions (e.g., Random Forest, Gradient Boosting, Stacking).\n",
    "   - Try voting classifiers for classification tasks.\n",
    "\n",
    "4. **Handle Class Imbalances**\n",
    "   - For imbalanced datasets, use techniques like:\n",
    "     - Oversampling (e.g., SMOTE)\n",
    "     - Undersampling\n",
    "     - Class weighting in the loss function:\n",
    "       ```python\n",
    "       model = RandomForestClassifier(class_weight='balanced')\n",
    "       ```\n",
    "\n",
    "5. **Feature Scaling**\n",
    "   - Standardize or normalize features to help models like SVM and k-NN perform better.\n",
    "\n",
    "6. **Cross-Validation**\n",
    "   - Use k-fold cross-validation to ensure model stability:\n",
    "     ```python\n",
    "     from sklearn.model_selection import cross_val_score\n",
    "     scores = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "     print(\"Average Accuracy:\", scores.mean())\n",
    "     ```\n",
    "\n",
    "7. **Outlier Treatment**\n",
    "   - Detect and handle outliers using Z-score or IQR methods.\n",
    "\n",
    "8. **Leverage Pretrained Models**\n",
    "   - For tasks like NLP or image classification, use transfer learning with models like BERT or ResNet.\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Considerations During the Hackathon**\n",
    "1. **Time Management**\n",
    "   - Allocate specific time blocks for preprocessing, model training, and tuning.\n",
    "   - Do not spend excessive time on a single model or step.\n",
    "\n",
    "2. **Baseline Model**\n",
    "   - Start with a baseline to compare improvements from preprocessing and feature engineering.\n",
    "\n",
    "3. **Pipeline Automation**\n",
    "   - Create preprocessing and model training pipelines for efficiency:\n",
    "     ```python\n",
    "     from sklearn.pipeline import Pipeline\n",
    "     pipeline = Pipeline([\n",
    "         ('imputer', SimpleImputer(strategy='mean')),\n",
    "         ('scaler', StandardScaler()),\n",
    "         ('model', RandomForestClassifier())\n",
    "     ])\n",
    "     ```\n",
    "\n",
    "4. **Validate on Provided Test Set**\n",
    "   - Once the test set is available, validate assumptions and ensure preprocessing steps generalize well.\n",
    "\n",
    "---\n",
    "\n",
    "### **Quick Wins for Accuracy**\n",
    "1. **Gradient Boosting Models (e.g., XGBoost, LightGBM)**\n",
    "   - They handle missing values and categorical features well.\n",
    "   ```python\n",
    "   from lightgbm import LGBMClassifier\n",
    "   model = LGBMClassifier(n_estimators=1000, learning_rate=0.05)\n",
    "   ```\n",
    "\n",
    "2. **Data Augmentation**\n",
    "   - Augment datasets (if applicable) to artificially increase size and diversity.\n",
    "\n",
    "3. **Early Stopping**\n",
    "   - Prevent overfitting by monitoring validation loss during training.\n",
    "\n",
    "4. **Custom Loss Functions**\n",
    "   - Define custom metrics that align with hackathon goals.\n",
    "\n",
    "5. **Feature Importance Analysis**\n",
    "   - Use feature importance plots from tree-based models to guide feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "Here are code snippets for testing multiple models for **classification**, **regression**, and **clustering** in one go.\n",
    "\n",
    "---\n",
    "\n",
    "### **1. Classification: Testing Multiple Models**\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Example Models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'SVC': SVC(),\n",
    "    'k-NN': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Sample data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\n",
    "\n",
    "# Test models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[name] = acc\n",
    "\n",
    "# Display results\n",
    "for model, score in results.items():\n",
    "    print(f\"{model}: Accuracy = {score:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Regression: Testing Multiple Models**\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Example Models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "# Sample data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Test models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    results[name] = mse\n",
    "\n",
    "# Display results\n",
    "for model, score in results.items():\n",
    "    print(f\"{model}: Mean Squared Error = {score:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Clustering: Testing Multiple Models**\n",
    "```python\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Example Models\n",
    "models = {\n",
    "    'K-Means': KMeans(n_clusters=3),\n",
    "    'DBSCAN': DBSCAN(eps=0.5, min_samples=5),\n",
    "    'Agglomerative Clustering': AgglomerativeClustering(n_clusters=3)\n",
    "}\n",
    "\n",
    "# Test models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    cluster_labels = model.fit_predict(X)\n",
    "    if len(set(cluster_labels)) > 1:  # Silhouette score requires >1 cluster\n",
    "        score = silhouette_score(X, cluster_labels)\n",
    "        results[name] = score\n",
    "    else:\n",
    "        results[name] = \"Cannot compute silhouette score (single cluster)\"\n",
    "\n",
    "# Display results\n",
    "for model, score in results.items():\n",
    "    print(f\"{model}: Silhouette Score = {score}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Generic Snippet: Evaluate Multiple Models**\n",
    "You can create a reusable function for any task:\n",
    "\n",
    "```python\n",
    "def evaluate_models(models, X_train, X_test, y_train, y_test, metric_func):\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        results[name] = metric_func(y_test, y_pred)\n",
    "    return results\n",
    "\n",
    "# Example Usage (Classification):\n",
    "from sklearn.metrics import f1_score\n",
    "models = {\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Logistic Regression': LogisticRegression()\n",
    "}\n",
    "results = evaluate_models(models, X_train, X_test, y_train, y_test, f1_score)\n",
    "print(results)\n",
    "```\n",
    "\n",
    "Here's how to pick the most accurate model and fine-tune its hyperparameters for better performance:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Identify the Best Model**\n",
    "After evaluating models, choose the one with the highest accuracy (or desired metric). For example:\n",
    "\n",
    "```python\n",
    "# Results from testing multiple models\n",
    "results = {\n",
    "    'Logistic Regression': 0.85,\n",
    "    'Random Forest': 0.89,\n",
    "    'SVC': 0.88,\n",
    "    'k-NN': 0.82\n",
    "}\n",
    "\n",
    "# Identify the best model\n",
    "best_model_name = max(results, key=results.get)\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Tune Hyperparameters**\n",
    "Use **GridSearchCV** or **RandomizedSearchCV** to optimize the selected model's hyperparameters.\n",
    "\n",
    "#### Example for Random Forest:\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the best model\n",
    "best_model = RandomForestClassifier()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after tuning\n",
    "tuned_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "#### Example for SVC:\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define the best model\n",
    "best_model = SVC()\n",
    "\n",
    "# Define hyperparameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],\n",
    "    'gamma': ['scale', 0.1, 0.01],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "# Perform Grid Search\n",
    "grid_search = GridSearchCV(estimator=best_model, param_grid=param_grid, \n",
    "                           cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model after tuning\n",
    "tuned_model = grid_search.best_estimator_\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Evaluate the Tuned Model**\n",
    "Test the tuned model on the validation/test set to ensure performance improvement:\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Evaluate tuned model\n",
    "y_pred = tuned_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of the tuned model: {accuracy:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Automate the Process (Optional)**\n",
    "You can automate the process of model selection and hyperparameter tuning:\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define models and their hyperparameters\n",
    "models = {\n",
    "    'Random Forest': (RandomForestClassifier(), {\n",
    "        'n_estimators': [50, 100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'min_samples_leaf': [1, 2]\n",
    "    }),\n",
    "    'SVC': (SVC(), {\n",
    "        'C': [0.1, 1, 10],\n",
    "        'gamma': ['scale', 0.1],\n",
    "        'kernel': ['linear', 'rbf']\n",
    "    })\n",
    "}\n",
    "\n",
    "# Test all models\n",
    "best_model, best_score, best_params = None, 0, None\n",
    "for model_name, (model, param_grid) in models.items():\n",
    "    search = RandomizedSearchCV(model, param_grid, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "    search.fit(X_train, y_train)\n",
    "    if search.best_score_ > best_score:\n",
    "        best_model, best_score, best_params = search.best_estimator_, search.best_score_, search.best_params_\n",
    "\n",
    "print(f\"Best Model: {best_model}\\nBest Score: {best_score}\\nBest Parameters: {best_params}\")\n",
    "```\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
